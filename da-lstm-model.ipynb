{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8585944,"sourceType":"datasetVersion","datasetId":5135321}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import required libraries\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy\n\nfrom sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, r2_score, mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport tensorflow as tf\nfrom tensorflow import keras, data\n\nfrom keras.layers import Input, LSTM, Attention, Dense, Layer\nfrom keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom random import random\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set Random Seeds to Get Reproducible Results\nseed_value = 0\n\n# Set `PYTHONHASHSEED` environment variable at a fixed value\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\n# Set `python` built-in pseudo-random generator at a fixed value\nimport random\nrandom.seed(seed_value)\n\n# Set `numpy` pseudo-random generator at a fixed value\nnp.random.seed(seed_value)\n\n# Set the `tensorflow` pseudo-random generator at a fixed value\ntf.random.set_seed(seed_value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    # IEEE PHM 2014 Data Challenge Dataset without ripples\ndf_part1 = pd.read_csv(r'../input/ieee2014/IEEE2014DataChallengeData/FC1_Without_Ripples/FC1_Ageing_part1.csv', encoding = \"ISO-8859-1\")\ndf_part2 = pd.read_csv(r'../input/ieee2014/IEEE2014DataChallengeData/FC1_Without_Ripples/FC1_Ageing_part2.csv', encoding = \"ISO-8859-1\")\ndf_part3 = pd.read_csv(r'../input/ieee2014/IEEE2014DataChallengeData/FC1_Without_Ripples/FC1_Ageing_part3.csv', encoding = \"ISO-8859-1\")\nframes = [df_part1, df_part2, df_part3]\nphm_dataset_1 = pd.concat(frames, ignore_index = True).astype('float32')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Elucidate statistical features\nphm_dataset_1.describe().transpose()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot stack voltage as a function of time\nplt.plot(list(phm_dataset_1['Time (h)']), list(phm_dataset_1['Utot (V)']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IEEE PHM 2014 Data Challenge Dataset with ripples\ndf_part1 = pd.read_csv(r'../input/ieee2014/IEEE2014DataChallengeData/Full_FC2_With_Ripples/FC2_Ageing_part1.csv', encoding = \"ISO-8859-1\")\ndf_part2 = pd.read_csv(r'../input/ieee2014/IEEE2014DataChallengeData/Full_FC2_With_Ripples/FC2_Ageing_part2.csv', encoding = \"ISO-8859-1\")\nphm_dataset_2 = pd.concat([df_part1, df_part2], ignore_index = True).dropna(axis=1, how='all').astype('float32')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Elucidate statistical features\nphm_dataset_2.describe().transpose()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot stack voltage as a function of time\nplt.plot(list(phm_dataset_2['Time (h)']), list(phm_dataset_2['Utot (V)']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def series_to_supervised(data, n_in = 1, n_out = 1, dropnan = True):\n    \"\"\"\n    Multivariate Time Series\n    Convert series to supervised learning\n    Standard practice in time series forecasting to use lagged observations (e.g. t-1) as input variables to forecast the current time step (t)\n    Code sourced from https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n    \"\"\"\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = pd.DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    agg = pd.concat(cols, axis = 1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace = True)\n    return agg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Implementation","metadata":{}},{"cell_type":"code","source":"class SelfAttention(Layer):\n    def __init__(self, units):\n        super(SelfAttention, self).__init__()\n        self.units = units\n\n    def build(self, input_shape):\n        self.W = self.add_weight(shape=(input_shape[-1], self.units),\n                                 initializer='random_normal',\n                                 trainable=True)\n        self.b = self.add_weight(shape=(self.units,),\n                                 initializer='zeros',\n                                 trainable=True)\n        self.u = self.add_weight(shape=(self.units,),\n                                 initializer='random_normal',\n                                 trainable=True)\n\n    def call(self, inputs):\n        # inputs.shape = (batch_size, time_steps, input_dim)\n        score = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=[2, 0]) + self.b)\n        score = tf.tensordot(score, self.u, axes=[2, 0])\n        attention_weights = tf.nn.softmax(score, axis=1)\n\n        # Multiply attention weights with the inputs to get the context vector\n        context_vector = attention_weights[..., tf.newaxis] * inputs\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        \n        # Return a 3D output\n        context_vector = tf.expand_dims(context_vector, 1)\n        context_vector = tf.tile(context_vector, [1, inputs.shape[1], 1])\n        \n        return context_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 96\nclass DA_LSTM():\n    \"\"\"\n    Implements structure, training, and validation of neural network models.\n    \"\"\"\n    def __init__(self, x_train, y_train, x_valid, y_valid, x_test, y_test, volt_scaler) -> None:\n        \"\"\"\n        volt_scaler is the MinMaxScaler object used to normalize Utot(V) in range (0,1)\n        \"\"\"\n        self.mae = keras.losses.MeanAbsoluteError()\n        self.mse = keras.losses.MeanSquaredError()\n        self.x_train, self.y_train = x_train, y_train  # Training set\n        self.x_valid, self.y_valid = x_valid, y_valid # Validation set\n        self.x_test, self.y_test = x_test, y_test   # Test set\n        self.volt_scaler = volt_scaler\n        # Hyperparameter tuning results for static loading dataset with 50% data as training set\n        # stored as attributes for easy accessibility\n        self.opt_LSTM_LR = 0.001 #Adam\n        \n    def build_LSTM_model(self, activator, optimizer):\n        \"\"\"\n        LSTM Network.\n        Use bidir_LSTM_layer in keras.Sequential() to implement a Bidirectional LSTM.\n        \"\"\"\n        keras.backend.clear_session()\n        output_neurons = 1\n        input_layer = Input(shape=(self.x_train.shape[1], self.x_train.shape[2]))\n        lstm_layer_1 = LSTM(256, return_sequences=True, activation=activator)(input_layer)\n\n        # Self-attention layer after lstm_layer_1\n        lstm_layer_1_attention = SelfAttention(units=16)(lstm_layer_1)\n        \n        lstm_layer_2 = LSTM(256, return_sequences=True, activation=activator, dropout=0.4)(lstm_layer_1_attention)\n        lstm_layer_3 = LSTM(128, return_sequences=True, activation=activator)(lstm_layer_2)\n        \n        # Scaled dot-product attention layer       \n        lstm_layer_3_attention = Attention()([lstm_layer_3, lstm_layer_3])\n        \n        lstm_layer_4 = LSTM(128, activation=activator, dropout=0.4)(lstm_layer_3_attention)\n\n        output_layer = Dense(output_neurons)(lstm_layer_4)\n\n        lstm_model = Model(inputs=input_layer, outputs=output_layer)\n#         lstm_model.reset_states()\n        for layer in lstm_model.layers:\n            if hasattr(layer, 'reset_states'):\n                layer.reset_states()\n        lstm_model.compile(optimizer = optimizer, loss = self.mse)\n        lstm_model.summary()\n        return lstm_model\n\n\n\n    def train_model(self, model, epoch_size = 150, BATCH_SIZE = BATCH_SIZE):\n        \"\"\"\n        Train the model with specified epochs and batch size.\n        Default values of epoch_size and BATCH_SIZE used when not supplied.\n        \"\"\"\n        early_stop = EarlyStopping(monitor = 'val_loss', patience = 50, mode = 'min', restore_best_weights=True)\n        \n        history = model.fit(self.x_train, self.y_train, epochs = epoch_size, batch_size = BATCH_SIZE, callbacks = [early_stop],\n                validation_data=  (self.x_valid, self.y_valid), shuffle = False)\n        \n        return history\n\n\n    def predict(self, model):\n        \"\"\"\n        model: DNN model built\n        volt_scaler: Scaler used to fit independent variable (stack voltage) data during initial data processing\n        \"\"\"\n        # Predict\n        yhat = model.predict(self.x_test)\n        x_test = self.x_test.reshape((self.x_test.shape[0], self.x_test.shape[2]))\n        inv_yhat = np.concatenate((yhat, x_test[:, 1:]), axis=1)\n        inv_yhat = self.volt_scaler.inverse_transform(inv_yhat)\n        inv_yhat = inv_yhat[:,0]\n        # invert scaling for actual\n        y_test = self.y_test.reshape((len(self.y_test), 1))\n        inv_y = np.concatenate((y_test, x_test[:, 1:]), axis=1)\n        inv_y = self.volt_scaler.inverse_transform(inv_y)\n        inv_y = inv_y[:,0]\n        return inv_y, inv_yhat\n\n\n    def plot_loss_curves(self, history, epoch_size):\n        \"\"\"\n        Visualize Model Results\n        \"\"\"\n        training_loss = history.history['loss']\n        validation_loss = history.history['val_loss']\n        epochs = range(1, epoch_size + 1)\n        plt.figure()\n        plt.plot(epochs, training_loss, label = 'Training Loss')\n        plt.plot(epochs, validation_loss, label = 'Validation Loss')\n        plt.legend()\n        plt.show()\n        plt.savefig('training.png')\n\n\n    def evaluate_metrics(self, y_true, y_pred):\n        \"\"\"\n        Return RMSE\n        \"\"\"\n        rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n        \n        return rmse\n\n\n    def evaluate_loss_function(self, y_true, y_pred):\n        \"\"\"\n        RMSE is chosen as the loss function for hyperparameter optimization\n        \"\"\"\n        return math.sqrt(mean_squared_error(y_true, y_pred))\n    \n    def plot_prediction_results(self, y_train, y_valid, y_test, y_true, y_pred):\n        \"\"\"\n        Plot the predicted voltage as a function of time\n        \"\"\"\n        training_time_arr = [_ for _ in range(len(y_train))]\n        validation_time_arr = [_ for _ in range(len(y_train), len(y_train) + len(y_valid))]\n        prediction_time_arr = [_ for _ in range(len(y_train) + len(y_valid), len(y_train) + len(y_valid) + len(y_test))]\n        plt.axvline(x = training_time_arr[0], linestyle = '--')\n        plt.axvline(x = training_time_arr[-1], linestyle = '--')\n        plt.axvline(x = validation_time_arr[-1], linestyle = '--')\n        plt.ylabel('Stack Voltage (V)')\n        plt.xlabel('Time (h)')\n        plt.plot(training_time_arr, y_train, label = 'Training voltage', color=(0, 0.8, 1, 0.4))\n        plt.plot(validation_time_arr, y_valid, label = 'Validation voltage', color=(1, 0, 0.80, 0.4))\n        plt.plot(prediction_time_arr, y_true, label = 'True voltage', color=(0, 1, 0, 0.6))\n        plt.plot(prediction_time_arr, y_pred, label = 'Predicted voltage', color=(1, 0, 0, 0.6))\n        plt.legend(loc = 'upper right', prop = {'size': 8})\n        # Download the image\n        plt.savefig('result.png')\n\n\n    def loop_model(self, n_loops = 50, df_type = 2):\n        \"\"\"\n        Outputs the RMSE, and RUL Prediction RE over 50 trials as arrays\n        Loop - Min, Max, Average of RMSE, predicted RUL, and relative error of predicted RUL over number of trials, default 50\n        df_type = 1 corresponds to the static loading dataset\n        df_type = 2 corresponds to the dynamic loading dataset\n        Uncomment the line corresponding to the model you wish to evaluate\n        \"\"\"\n        arr_RMSE, arr_RUL, arr_RUL_RE = list(), list(), list()\n        for _ in range(n_loops):\n            print(\"Trial \", _+1 ,\" of\", n_loops)\n            # build the model\n            model = self.build_LSTM_model('tanh', tf.keras.optimizers.Adam(learning_rate = self.opt_LSTM_LR))\n\n            # training the model\n            history = self.train_model(model, epoch_size = 150, BATCH_SIZE = BATCH_SIZE)\n        \n            #plot loss curves\n            self.plot_loss_curves(history, epoch_size = len(history.history['loss']))\n            \n            # make prediction\n            y_true, y_pred = self.predict(model)\n            \n            #evaluating the model\n            rmse = self.evaluate_metrics(y_true, y_pred)\n            \n            # initial voltage for threshold chosen as first voltage in training set of each dataset\n            threshold_4perc = 3.1971 if df_type == 2 else 3.2028\n            validation_h = len(self.y_valid)\n            true_failure_h = [i for i,v in enumerate(y_true) if v < threshold_4perc][0]\n            true_RUL = validation_h + true_failure_h\n            try:\n                pred_failure_h = [i for i,v in enumerate(y_pred) if v < threshold_4perc][0]\n            except IndexError:\n                pred_failure_h = len(y_pred)\n            \n            # RUL = validation set time + time to failure\n            pred_RUL = validation_h + pred_failure_h\n            \n            \n            arr_RUL.append(pred_RUL) \n            arr_RUL_RE.append(abs(pred_RUL - true_RUL)/true_RUL)\n            print(f\"RMSE: {rmse}\")\n            arr_RMSE.append(rmse)\n            \n            #plot the prediction results\n            self.plot_prediction_results(self.volt_scaler.inverse_transform(self.y_train), self.volt_scaler.inverse_transform(self.y_valid), self.y_test, y_true, y_pred)\n            \n            del model\n            keras.backend.clear_session()\n\n        arr_RMSE, arr_RUL, arr_RUL_RE = np.array(arr_RMSE), np.array(arr_RUL), np.array(arr_RUL_RE)\n        min_RMSE, min_RUL, min_RUL_RE = arr_RMSE.min(), arr_RUL.min(), arr_RUL_RE.min()\n        max_RMSE, max_RUL, max_RUL_RE = arr_RMSE.max(), arr_RUL.max(), arr_RUL_RE.max()\n        avg_RMSE, avg_RUL, avg_RUL_RE = arr_RMSE.mean(), arr_RUL.mean(), arr_RUL_RE.mean()\n        \n        print(\"RMSE Array\", arr_RMSE, end = '\\n')\n        print(\"RUL Array\", arr_RUL, end = '\\n')\n        print(\"RUL_RE Array\", arr_RUL_RE, end = '\\n')\n        print(f\"Mean RMSE: {avg_RMSE}, Min RMSE: {min_RMSE}, Max RMSE: {max_RMSE}\")\n        print(f\"True RUL: {true_RUL}, Min RUL: {min_RUL}, Mean RUL: {avg_RUL}, Max RUL: {max_RUL}\")\n        print(f\"Min RUL_RE: {min_RUL_RE}, Mean RUL_RE: {avg_RUL_RE}, Max RUL_RE: {max_RUL_RE}\")\n        \n        return arr_RMSE, arr_RUL, arr_RUL_RE, y_pred, y_true","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset Processor Class","metadata":{}},{"cell_type":"code","source":"class df_handler():\n    \"\"\"\n    Class implementing downsampling, Savitzky-Golay filtering,\n    \"\"\"\n    def __init__(self, dataset: pd.DataFrame):\n        self.fc_dataset = dataset\n\n\n    def reconstruct_dataset(self):\n        \"\"\"\n        Downsample the dataset at 1 h interval.\n        Returns the sampled time indices, stack voltage, dataset, and features\n        \"\"\"\n        time_original = self.fc_dataset['Time (h)']\n        sampled_indexes = self.fc_dataset.astype('int32').drop_duplicates(subset = ['Time (h)']).index\n        sampled_df = self.fc_dataset.iloc[sampled_indexes, :]\n        time_h = sampled_df['Time (h)']\n        volt_total = sampled_df['Utot (V)'].values.tolist()\n        features = sampled_df.copy(deep = True).drop(labels = ['Time (h)', 'Utot (V)'], axis = 1)\n        return time_h, volt_total, sampled_df, features\n\n\n    def smooth_dataset(self, sampled_df):\n        \"\"\"\n        Smooths the (sampled) dataset by Savitzky-Golay smoothing\n        The window length and polynomial order may be tweaked by the variables window_length and polynomial_order, respectively.\n        \"\"\"\n        window_length = 21\n        polynomial_order = 2\n        smoothed_dataset = pd.DataFrame()\n        for col in sampled_df.columns[1:]: #Exclude Time (h) column\n            smoothed_dataset[col] = scipy.signal.savgol_filter(sampled_df[col], window_length, polynomial_order)\n        return smoothed_dataset\n\n\n    def train_test_split(self, train_frac, smoothed_dataset):\n        \"\"\"\n        Splits dataset into train-validation-test sets after smoothing features\n        50% (520 h) train, 10% validation (115 h), 40% test (519 h) for FC1: wihout ripple dataset\n        50% (460 h) train, 10% validation (102 h), 40% test (458 h) for FC2: ripple dataset\n        train_frac: fraction of data to be used for training; of the remaining datset, 10% is for validation and the rest is the test set\n        \"\"\"\n        sup_df = series_to_supervised(smoothed_dataset)\n        num_cols = len(smoothed_dataset.columns)\n        n = len(sup_df)\n        \n        # Drop variables not to predict at the next time step; predict only the stack's total voltage.\n        # Do not drop column 29 with output voltage\n        sup_dataset = sup_df.drop(columns = sup_df.columns[list(_ for _ in range(num_cols, 2*num_cols) if _ != 29)], axis = 1)\n        sup_values = sup_dataset.values\n        sup_train, sup_valid, sup_test = sup_values[:int(n*train_frac),:], sup_values[int(n*train_frac):int(n*(train_frac+0.1)),:], sup_values[int(n*(train_frac+0.1)):,:]\n        features_train, features_valid, features_test = sup_train[:, :-1], sup_valid[:, :-1], sup_test[:, :-1]\n        train_y, valid_y, test_y = sup_train[:, -1], sup_valid[:, -1], sup_test[:, -1]\n        return features_train, train_y, features_valid, valid_y, features_test, test_y\n\n\n    def normalize_split_sets(self, features_train, features_valid, features_test, train_y, valid_y, test_y):\n        \"\"\"\n        Normalize the dataset features in the range (0, 1)\n        \"\"\"\n        # Normalize features\n        feature_scaler = MinMaxScaler(feature_range = (0, 1))\n        normalized_train_features = feature_scaler.fit_transform(features_train)\n        normalized_valid_features = feature_scaler.transform(features_valid)\n        normalized_test_features = feature_scaler.transform(features_test)\n        normalized_train_features = normalized_train_features.reshape((normalized_train_features.shape[0], 1, normalized_train_features.shape[1]))\n        normalized_valid_features = normalized_valid_features.reshape((normalized_valid_features.shape[0], 1, normalized_valid_features.shape[1]))\n        normalized_test_features = normalized_test_features.reshape((normalized_test_features.shape[0], 1, normalized_test_features.shape[1]))\n        \n        # Normalize output voltage\n        volt_scaler = MinMaxScaler(feature_range = (0, 1))\n        normalized_train_volt = volt_scaler.fit_transform(train_y.reshape(train_y.shape[0], 1))\n        normalized_valid_volt = volt_scaler.transform(valid_y.reshape(valid_y.shape[0], 1))\n        normalized_test_volt = volt_scaler.transform(test_y.reshape(test_y.shape[0], 1))\n        return (normalized_train_features, normalized_train_volt, normalized_valid_features, normalized_valid_volt,\n                              normalized_test_features, normalized_test_volt, volt_scaler)\n\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess the Dataset","metadata":{}},{"cell_type":"code","source":"# Reconstruct and smooth dataset 1\nfc1 = df_handler(phm_dataset_1)\ntime_1, volt_1, df_1, features_1 = fc1.reconstruct_dataset()\nsmoothed_df_1 = fc1.smooth_dataset(df_1)\nx_train_1, y_train_1, x_valid_1, y_valid_1, x_test_1, y_test_1 = fc1.train_test_split(0.5, smoothed_df_1)\nx_train_s1, y_train_s1, x_valid_s1, y_valid_s1, x_test_s1, y_test_s1, volt_scaler_1 = \\\n                        fc1.normalize_split_sets(x_train_1, x_valid_1, x_test_1, y_train_1, y_valid_1, y_test_1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reconstruct and smooth dataset 2\nfc2 = df_handler(phm_dataset_2)\ntime_2, volt_2, df_2, features_2 = fc2.reconstruct_dataset()\nsmoothed_df_2 = fc2.smooth_dataset(df_2)\nx_train_2, y_train_2, x_valid_2, y_valid_2, x_test_2, y_test_2 = fc2.train_test_split(0.5, smoothed_df_2)\nx_train_s2, y_train_s2, x_valid_s2, y_valid_s2, x_test_s2, y_test_s2, volt_scaler_2 = \\\n                        fc2.normalize_split_sets(x_train_2, x_valid_2, x_test_2, y_train_2, y_valid_2, y_test_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## PLOT FC1 RAW, FILTERED, AND SAMPLED VOLTAGES ##\nplt.figure()\nplt.plot(phm_dataset_1['Time (h)'], phm_dataset_1['Utot (V)'], color=(0, 0.8, 1, 0.4), label = 'Raw voltage')\nplt.plot(time_1, volt_1, color=(1, 0, 0.80, 0.4), label = 'Sampled voltage')\nplt.plot(time_1, smoothed_df_1['Utot (V)'], color=(0, 0.8, 0, 0.4), label = 'Filtered voltage')\nplt.xlabel('Time (h)')\nplt.ylabel('Stack Voltage (V)')\nplt.legend(loc = 'upper right')\nplt.grid(visible=True, which='major', color = 'darkgray', lw = 0.2)\n# Download the figure\nplt.savefig('time vs stack for FC1.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## PLOT FC2 RAW, FILTERED, AND SAMPLED VOLTAGES ##\nplt.figure()\nplt.plot(phm_dataset_2['Time (h)'], phm_dataset_2['Utot (V)'], color=(0, 0.8, 1, 0.4), label = 'Raw voltage')\nplt.plot(time_2, volt_2, color=(1, 0, 0.80, 0.4), label = 'Sampled voltage')\nplt.plot(time_2, smoothed_df_2['Utot (V)'],  color=(0, 0.8, 0, 0.4), label = 'Filtered voltage')\nplt.xlabel('Time (h)')\nplt.ylabel('Stack Voltage (V)')\nplt.legend(loc = 'upper right')\nplt.grid(visible=True, which='major', color = 'darkgray', lw = 0.2)\n# Download the figure\nplt.savefig('time vs stack for FC2.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Run to evaluate model performance\nmethod = DA_LSTM(x_train_s1, y_train_s1, x_valid_s1, y_valid_s1, x_test_s1, y_test_s1, volt_scaler_1)\n\n# Evaluate RMSE\narr_RMSE, arr_RUL, arr_RUL_RE, y_pred, y_true = method.loop_model(1,2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
